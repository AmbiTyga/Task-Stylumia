{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model-Training and Testing",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmbiTyga/Task-Stylumia/blob/Basic/Model-Training_and_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASqB2wYxxZQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9a81ce-917d-46be-a34c-6cc0f90f181e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/AmbiTyga/Task-Stylumia/Basic/TrainTest.7z"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-28 11:11:33--  https://raw.githubusercontent.com/AmbiTyga/Task-Stylumia/Basic/TrainTest.7z\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9147130 (8.7M) [application/octet-stream]\n",
            "Saving to: ‘TrainTest.7z’\n",
            "\n",
            "TrainTest.7z        100%[===================>]   8.72M  30.4MB/s    in 0.3s    \n",
            "\n",
            "2021-02-28 11:12:37 (30.4 MB/s) - ‘TrainTest.7z’ saved [9147130/9147130]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh4t8ETbx_u1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8040a6d-c95c-4679-ad4e-5dbb11352dbc"
      },
      "source": [
        "!7z x TrainTest.7z"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,4 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 9147130 bytes (8933 KiB)\n",
            "\n",
            "Extracting archive: TrainTest.7z\n",
            "--\n",
            "Path = TrainTest.7z\n",
            "Type = 7z\n",
            "Physical Size = 9147130\n",
            "Headers Size = 194\n",
            "Method = LZMA2:24\n",
            "Solid = +\n",
            "Blocks = 1\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b 26% 1 - train_parsed.json\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 1 - train_parsed.json\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b100% 2\b\b\b\b\b\b      \b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 2\n",
            "Size:       109002701\n",
            "Compressed: 9147130\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psECtCenyDMf"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch \r\n",
        "from torchtext import data\r\n",
        "from torchtext.vocab import GloVe\r\n",
        "import torch.nn as nn\r\n",
        "import re"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0geNMvkiCZx"
      },
      "source": [
        "train = pd.read_json('/content/train_parsed.json')\r\n",
        "test = pd.read_json('/content/test_parsed.json')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtN8oszHIFjg"
      },
      "source": [
        "def padding(text,attr = 'title'):\r\n",
        "  max_len = 39 if attr=='title' else 282\r\n",
        "  sent_len = len(text.split())\r\n",
        "  if sent_len>max_len:\r\n",
        "    return \" \".join(text.split()[:max_len])\r\n",
        "  else:\r\n",
        "    text = \" \".join(text.split()+['<pad>']*(max_len-sent_len))\r\n",
        "    return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFhq5ommic5S"
      },
      "source": [
        "train.drop(columns=['tld_with_tok', 'ac_tok','title_tok', 'body_tok', 'text', 'raw_text'],inplace = True)\r\n",
        "test.drop(columns=['tld_with_tok', 'ac_tok','title_tok', 'body_tok', 'text', 'raw_text'],inplace = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLlhvwrpOr5F"
      },
      "source": [
        "train['text'] = '<tld> '+ train['tld']+ ' <ac> ' + train['alchemy_category'] + ' <title> ' + train['title'].apply(padding)+ ' <body> ' + train['body'].apply(padding,attr='body') \r\n",
        "test['text'] = '<tld> '+ test['tld']+ ' <ac> ' + test['alchemy_category'] + ' <title> ' + test['title'].apply(padding)+ ' <body> ' + test['body'].apply(padding,attr='body') "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Fo08pASffw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "Train,Val = train_test_split(train,random_state = 2021,test_size = 0.2,stratify = train['label'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjqZJA05iYWM",
        "outputId": "33dda0a4-42a2-4656-eebd-09f5d70c33ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train.select_dtypes('object').columns"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tld', 'alchemy_category', 'title', 'body', 'text'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gth54lTdS9yd"
      },
      "source": [
        "Train.to_csv('train.csv',index= False)\r\n",
        "Val.to_csv('val.csv',index= False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xya6fDS2JuK"
      },
      "source": [
        "TEXT =  data.Field()\r\n",
        "fields = [(None,None) if x !='text' else ('text',TEXT) for x in train.columns]\r\n",
        "LABEL = data.LabelField()\r\n",
        "fields[26] = ('label',LABEL)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHYvPYNraZai"
      },
      "source": [
        "train_data, valid_data = data.TabularDataset.splits(\r\n",
        "                            path = '/content',\r\n",
        "                            train = 'train.csv',\r\n",
        "                            validation = 'val.csv',\r\n",
        "                            format = 'csv',\r\n",
        "                            fields = fields)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LW6Og2pkaf8"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3siEKv1gn6_"
      },
      "source": [
        "TEXT.build_vocab(train_data,vectors = GloVe())\r\n",
        "TEXT.build_vocab(valid_data,vectors = GloVe())\r\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIhN-hQ3ikwp"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2GW0xcpiIHy"
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ilAih5nhayX"
      },
      "source": [
        "device = torch.device('cuda')\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "train_iterator, valid_iterator = data.Iterator.splits(\r\n",
        "    (train_data, valid_data),\r\n",
        "    sort = False,\r\n",
        "    shuffle = False,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    device=device)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTiirvGFjkpK"
      },
      "source": [
        "class NumericalDataset:\r\n",
        "  def __init__(self,data,train=True):\r\n",
        "    self.features = ['alchemy_category_score', 'alchemy_labels', 'avglinksize',\r\n",
        "       'commonlinkratio_1', 'commonlinkratio_2', 'commonlinkratio_3',\r\n",
        "       'commonlinkratio_4', 'compression_ratio', 'embed_ratio',\r\n",
        "       'frameTagRatio', 'hasDomainLink', 'html_ratio', 'image_ratio',\r\n",
        "       'is_news', 'lengthyLinkDomain', 'linkwordscore',\r\n",
        "       'non_markup_alphanum_characters', 'numberOfLinks', 'numwords_in_url',\r\n",
        "       'parametrizedLinkRatio', 'spelling_errors_ratio']\r\n",
        "    self.data = data\r\n",
        "    self.flag = train\r\n",
        "\r\n",
        "  def __getitem__(self,i):\r\n",
        "    X = self.data.loc[i,self.features].astype(float).values\r\n",
        "    y = self.data.loc[i,'label']\r\n",
        "    return torch.DoubleTensor(X),torch.LongTensor([y])\r\n",
        "  \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.data)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdEg0wLtFr_V"
      },
      "source": [
        "linear_data = NumericalDataset(train)\r\n",
        "linear_data_loader = torch.utils.data.DataLoader(linear_data,batch_size=32)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me3RY0aGhCoy"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.autograd import Variable\r\n",
        "from torch.nn import functional as F\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class AttentionModel(torch.nn.Module):\r\n",
        "  def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\r\n",
        "    super(AttentionModel, self).__init__()\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Arguments\r\n",
        "    ---------\r\n",
        "    batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\r\n",
        "    output_size : 2 = (pos, neg)\r\n",
        "    hidden_sie : Size of the hidden_state of the LSTM\r\n",
        "    vocab_size : Size of the vocabulary containing unique words\r\n",
        "    embedding_length : Embeddding dimension of GloVe word embeddings\r\n",
        "    weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \r\n",
        "\r\n",
        "    --------\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.output_size = output_size\r\n",
        "    self.hidden_size = hidden_size\r\n",
        "    self.vocab_size = vocab_size\r\n",
        "    self.embedding_length = embedding_length\r\n",
        "\r\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\r\n",
        "    self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\r\n",
        "    self.lstm = nn.LSTM(embedding_length, hidden_size)\r\n",
        "    self.label = nn.Linear(hidden_size, output_size)\r\n",
        "    #self.attn_fc_layer = nn.Linear()\r\n",
        "    \r\n",
        "  def attention_net(self, lstm_output, final_state):\r\n",
        "\r\n",
        "    \"\"\" \r\n",
        "    Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\r\n",
        "    between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\r\n",
        "\r\n",
        "    Arguments\r\n",
        "    ---------\r\n",
        "\r\n",
        "    lstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\r\n",
        "    final_state : Final time-step hidden state (h_n) of the LSTM\r\n",
        "\r\n",
        "    ---------\r\n",
        "\r\n",
        "    Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\r\n",
        "          new hidden state.\r\n",
        "          \r\n",
        "    Tensor Size :\r\n",
        "          hidden.size() = (batch_size, hidden_size)\r\n",
        "          attn_weights.size() = (batch_size, num_seq)\r\n",
        "          soft_attn_weights.size() = (batch_size, num_seq)\r\n",
        "          new_hidden_state.size() = (batch_size, hidden_size)\r\n",
        "            \r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    hidden = final_state.squeeze(0)\r\n",
        "    attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\r\n",
        "    soft_attn_weights = F.softmax(attn_weights, 1)\r\n",
        "    new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\r\n",
        "\r\n",
        "    return new_hidden_state\r\n",
        "\r\n",
        "  def forward(self, input_sentences, batch_size=None):\r\n",
        "\r\n",
        "    \"\"\" \r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    input_sentence: input_sentence of shape = (batch_size, num_sequences)\r\n",
        "    batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    Output of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\r\n",
        "    final_output.shape = (batch_size, output_size)\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    input = self.word_embeddings(input_sentences)\r\n",
        "    print(input.shape)\r\n",
        "    input = input.permute(1, 0, 2)\r\n",
        "    if batch_size is None:\r\n",
        "      h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\r\n",
        "      c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda())\r\n",
        "    else:\r\n",
        "      h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\r\n",
        "      c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\r\n",
        "      \r\n",
        "    output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \r\n",
        "    output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\r\n",
        "\r\n",
        "    attn_output = self.attention_net(output, final_hidden_state)\r\n",
        "    logits = self.label(attn_output)\r\n",
        "\r\n",
        "    return logits"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG0BdgsahJ4q",
        "outputId": "18a1f704-ecc8-45c4-9e01-4be420c2b922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "def clip_gradient(model, clip_value):\r\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\r\n",
        "    for p in params:\r\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\r\n",
        "    \r\n",
        "def train_model(model, train_iter, epoch):\r\n",
        "    total_epoch_loss = 0\r\n",
        "    total_epoch_acc = 0\r\n",
        "    model.cuda()\r\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\r\n",
        "    steps = 0\r\n",
        "    model.train()\r\n",
        "    for idx, batch in enumerate(train_iter):\r\n",
        "        text = batch.text[0]\r\n",
        "        target = batch.label\r\n",
        "        target = torch.autograd.Variable(target).long()\r\n",
        "        if torch.cuda.is_available():\r\n",
        "            text = text.cuda()\r\n",
        "            target = target.cuda()\r\n",
        "        if (text.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\r\n",
        "            continue\r\n",
        "        optim.zero_grad()\r\n",
        "        prediction = model(text)\r\n",
        "        loss = loss_fn(prediction, target)\r\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\r\n",
        "        acc = 100.0 * num_corrects/len(batch)\r\n",
        "        loss.backward()\r\n",
        "        clip_gradient(model, 1e-1)\r\n",
        "        optim.step()\r\n",
        "        steps += 1\r\n",
        "        \r\n",
        "        if steps % 100 == 0:\r\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\r\n",
        "        \r\n",
        "        total_epoch_loss += loss.item()\r\n",
        "        total_epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\r\n",
        "\r\n",
        "def eval_model(model, val_iter):\r\n",
        "    total_epoch_loss = 0\r\n",
        "    total_epoch_acc = 0\r\n",
        "    model.eval()\r\n",
        "    with torch.no_grad():\r\n",
        "        for idx, batch in enumerate(val_iter):\r\n",
        "            text = batch.text[0]\r\n",
        "            if (text.size()[0] is not 32):\r\n",
        "                continue\r\n",
        "            target = batch.label\r\n",
        "            target = torch.autograd.Variable(target).long()\r\n",
        "            if torch.cuda.is_available():\r\n",
        "                text = text.cuda()\r\n",
        "                target = target.cuda()\r\n",
        "            prediction = model(text)\r\n",
        "            loss = loss_fn(prediction, target)\r\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\r\n",
        "            acc = 100.0 * num_corrects/len(batch)\r\n",
        "            total_epoch_loss += loss.item()\r\n",
        "            total_epoch_acc += acc.item()\r\n",
        "\r\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\r\n",
        "\t\r\n",
        "\r\n",
        "learning_rate = 2e-5\r\n",
        "batch_size = 32\r\n",
        "output_size = 2\r\n",
        "hidden_size = 256\r\n",
        "embedding_length = 300\r\n",
        "word_embeddings = TEXT.vocab.vectors\r\n",
        "vocab_size = len(TEXT.vocab)\r\n",
        "\r\n",
        "model = AttentionModel(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\r\n",
        "loss_fn = F.cross_entropy\r\n",
        "\r\n",
        "for epoch in range(10):\r\n",
        "    train_loss, train_acc = train_model(model, train_iterator, epoch)\r\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\r\n",
        "    \r\n",
        "test_loss, test_acc = eval_model(model, val_iterator)\r\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\r\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 300])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-91193c1cca87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-91193c1cca87>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iter, epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnum_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-b15aba77a384>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_sentences, batch_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0mh_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxhbzCtSjxP5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}